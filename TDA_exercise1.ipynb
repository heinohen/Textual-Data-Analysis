{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "dwVt21_ZPu8_",
        "j8M7unpmTTIB",
        "MmLrBfmGfQJB"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EXERCISE TASK 1"
      ],
      "metadata": {
        "id": "lywdzR6ANPvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SETUP"
      ],
      "metadata": {
        "id": "6I4MszuIOUos"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "ut6_YpE5OBCw"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset,get_dataset_infos, Dataset"
      ],
      "metadata": {
        "id": "JG2AYG31N8G7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: loading a dataset from the HF hub"
      ],
      "metadata": {
        "id": "OPjVYdXvNUsA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `load_dataset` function of the `datasets` library, load each of the following datasets in turn`\n",
        "\n",
        "*    stanfordnlp/imdb\n",
        "*    eriktks/conll2003\n",
        "*    openai/gsm8k\n",
        "\n",
        "ANSWER QUESTIONS:\n",
        "\n",
        "* What NLP task is the dataset intended for (e.g. syntactic analysis, toxicity detection, etc.)? (You may need to refer to the documentation of the dataset for this.)\n",
        "* What parts is the dataset split into (e.g. train, test) and how many examples does each contain?\n",
        "* What features (e.g. text, label) does the dataset have? (Try to understand how these relate to the NLP task the dataset is intended for.)\n",
        "* What is the first item in the training set of the dataset?\n"
      ],
      "metadata": {
        "id": "MuaZuuS1Nfc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### stanfordnlp/imdb"
      ],
      "metadata": {
        "id": "dwVt21_ZPu8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_string = 'stanfordnlp/imdb'\n",
        "imdb = load_dataset(imdb_string)"
      ],
      "metadata": {
        "id": "rfP-Zh95O9TW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3213ef-415c-449e-efb4-38b1612e6252"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What NLP task is the dataset intended for (e.g. syntactic analysis, toxicity detection, etc.)? (You may need to refer to the documentation of the dataset for this.)"
      ],
      "metadata": {
        "id": "lkOBur-sP08n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "get_dataset_infos(imdb_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twAFLdrKPqBq",
        "outputId": "f89a58d5-ccfa-46d7-eb1a-d2fc4f54aca2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'plain_text': DatasetInfo(description='', citation='', homepage='', license='', features={'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}, post_processed=None, supervised_keys=None, builder_name='parquet', dataset_name='imdb', config_name='plain_text', version=0.0.0, splits={'train': SplitInfo(name='train', num_bytes=33435948, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'test': SplitInfo(name='test', num_bytes=32653810, num_examples=25000, shard_lengths=None, dataset_name='imdb'), 'unsupervised': SplitInfo(name='unsupervised', num_bytes=67113044, num_examples=50000, shard_lengths=None, dataset_name='imdb')}, download_checksums={'hf://datasets/stanfordnlp/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/train-00000-of-00001.parquet': {'num_bytes': 20979968, 'checksum': None}, 'hf://datasets/stanfordnlp/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/test-00000-of-00001.parquet': {'num_bytes': 20470363, 'checksum': None}, 'hf://datasets/stanfordnlp/imdb@e6281661ce1c48d982bc483cf8a173c1bbeb5d31/plain_text/unsupervised-00000-of-00001.parquet': {'num_bytes': 41996509, 'checksum': None}}, download_size=83446840, post_processing_size=None, dataset_size=133202802, size_in_bytes=216649642)}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Has no description, but it is (quote from hf dataset card: https://huggingface.co/datasets/stanfordnlp/imdb):\n",
        "\n",
        ">Large Movie Review Dataset. This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well."
      ],
      "metadata": {
        "id": "q9gIW8hJQVGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What parts is the dataset split into (e.g. train, test) and how many examples does each contain?"
      ],
      "metadata": {
        "id": "zOdHhypXRAXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD6cWXV8Q_hx",
        "outputId": "0bf2d877-46d7-4439-f812-d58a7a5cfec9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is split to train, test and unsupervised and holds 25000, 25000, 50000 examples respectively"
      ],
      "metadata": {
        "id": "8DzueLurRLaK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What features (e.g. text, label) does the dataset have? (Try to understand how these relate to the NLP task the dataset is intended for.)"
      ],
      "metadata": {
        "id": "7g60O8nDRu6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(imdb['train'][0]['text'])\n",
        "print(imdb['train'][0]['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vLONzJORyJP",
        "outputId": "2852c22d-9a70-449a-a81e-9b0f163f964e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset entries have features of `text` that represents the actual review text of the item and `label` as the representation of negative = 0, or positive = 1 review of the product"
      ],
      "metadata": {
        "id": "ROLNfurPSE3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the first item in the training set of the dataset?"
      ],
      "metadata": {
        "id": "PN1sTcIPRxd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imdb['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Upm69tt8RzMv",
        "outputId": "4125409e-d9ea-4872-b545-7a59518af55a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              " 'label': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### eriktks/conll2003"
      ],
      "metadata": {
        "id": "j8M7unpmTTIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll_string = 'eriktks/conll2003'\n",
        "conll = load_dataset(conll_string,trust_remote_code=True)"
      ],
      "metadata": {
        "id": "dCo7Pw2xTVZ3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What NLP task is the dataset intended for (e.g. syntactic analysis, toxicity detection, etc.)? (You may need to refer to the documentation of the dataset for this.)"
      ],
      "metadata": {
        "id": "UuJuvR3XTqog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conllinfo = get_dataset_infos(conll_string)\n",
        "print(conllinfo['conll2003'].description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWItytPoTrit",
        "outputId": "95341e21-344d-46cb-93d3-5ecfff80e5a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\n",
            "four types of named entities: persons, locations, organizations and names of miscellaneous entities that do\n",
            "not belong to the previous three groups.\n",
            "\n",
            "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\n",
            "a separate line and there is an empty line after each sentence. The first item on each line is a word, the second\n",
            "a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\n",
            "and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\n",
            "if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\n",
            "B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\n",
            "tagging scheme, whereas the original dataset uses IOB1.\n",
            "\n",
            "For more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What parts is the dataset split into (e.g. train, test) and how many examples does each contain?"
      ],
      "metadata": {
        "id": "_0tJUIkYVpzn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is split to train, validation and test and holds 14041, 3250, 3250 examples respectively"
      ],
      "metadata": {
        "id": "NNjaJYLiVryg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpV8BhCuVlle",
        "outputId": "b80d6ab7-83c1-43d5-8a02-95134088a66b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What features (e.g. text, label) does the dataset have? (Try to understand how these relate to the NLP task the dataset is intended for.)"
      ],
      "metadata": {
        "id": "cuMZaYD_V6An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dataset entries have features of id that represents the id of the example, tokens that belong to that sentence. part-of-speech (POS) tag\\\n"
      ],
      "metadata": {
        "id": "d2VknzsjV_oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the first item in the training set of the dataset?"
      ],
      "metadata": {
        "id": "S5i5M1aGX4K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conll['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imu2qhq0X46P",
        "outputId": "8fb7e71d-727c-4aaa-f35c-ab404111f6d3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### openai/gsm8k"
      ],
      "metadata": {
        "id": "MmLrBfmGfQJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_string = 'openai/gsm8k'\n",
        "gsm8k = load_dataset(gsm8k_string,\"main\",trust_remote_code=True)"
      ],
      "metadata": {
        "id": "mePCbZWjfhgR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What NLP task is the dataset intended for (e.g. syntactic analysis, toxicity detection, etc.)? (You may need to refer to the documentation of the dataset for this.)"
      ],
      "metadata": {
        "id": "6ZYwg6xHfWoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_info = get_dataset_infos(gsm8k_string)\n",
        "gsm8k_info['main'].description"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "3KUdCnmXgATz",
        "outputId": "808ccb3e-ffc1-4263-b15f-3f4248a37107"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wo2Qt76fh8M",
        "outputId": "b777a497-9d4a-4261-c5c8-7930202e7edb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer'],\n",
              "        num_rows: 7473\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'answer'],\n",
              "        num_rows: 1319\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What parts is the dataset split into (e.g. train, test) and how many examples does each contain?"
      ],
      "metadata": {
        "id": "ZF-LIvmbfazk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset is split to train and test and holds 7473 and 1319 examples respectively"
      ],
      "metadata": {
        "id": "Yd6vcmtJgn5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What features (e.g. text, label) does the dataset have? (Try to understand how these relate to the NLP task the dataset is intended for.)"
      ],
      "metadata": {
        "id": "pcCMq2k5fdzw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(gsm8k['train'][0]['question'])\n",
        "print(gsm8k['train'][0]['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oI8k4VlfiT5",
        "outputId": "aafdb7e4-53c7-4df0-9169-f97071a7173f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
            "Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
            "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
            "#### 72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the first item in the training set of the dataset?"
      ],
      "metadata": {
        "id": "RHrP3k0HfgBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GL253H4XfisI",
        "outputId": "b5fba429-cb74-4845-a1ea-b403eceee083"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?',\n",
              " 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: creating a dataset from your own data"
      ],
      "metadata": {
        "id": "M7TM5sySNXKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://dl.turkunlp.org/TKO_8964_2023/news-fi-2021.jsonl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CahGxlniDMr",
        "outputId": "60b4e704-d51e-40a7-c04b-421230a7c0de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-15 11:13:51--  http://dl.turkunlp.org/TKO_8964_2023/news-fi-2021.jsonl\n",
            "Resolving dl.turkunlp.org (dl.turkunlp.org)... 195.148.30.23\n",
            "Connecting to dl.turkunlp.org (dl.turkunlp.org)|195.148.30.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36139303 (34M) [application/octet-stream]\n",
            "Saving to: ‘news-fi-2021.jsonl.2’\n",
            "\n",
            "news-fi-2021.jsonl. 100%[===================>]  34.46M  8.69MB/s    in 4.0s    \n",
            "\n",
            "2025-01-15 11:13:55 (8.69 MB/s) - ‘news-fi-2021.jsonl.2’ saved [36139303/36139303]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"json\", data_files=\"news-fi-2021.jsonl\")\n",
        "ds = dataset['train'].train_test_split(test_size=0.2)"
      ],
      "metadata": {
        "id": "d5IgkKjpiE9_"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3or3CFtgjGg5",
        "outputId": "9c1cad38-04ed-4d06-8146-61bff6aef5d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['summary', 'tags', 'text', 'timestamp', 'title', 'url'],\n",
              "        num_rows: 5985\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['summary', 'tags', 'text', 'timestamp', 'title', 'url'],\n",
              "        num_rows: 1497\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX_qPQI_mdqw",
        "outputId": "c8da1c50-0b08-4702-c504-b2d56347cfc6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'Vaikka osalla abeista on jo kaksi koronarokotusta, tänään maanantaina alkavien ylioppilaskirjoitusten koronasäännöt pysyvät tiukkoina: osallistua saa vain oireettomana. Kahdesti rokotetulla on kuitenkin mahdollisuus välttyä karanteenilta.',\n",
              " 'tags': ['Ennaltaehkäisevä terveydenhuolto',\n",
              "  'Etelä-Pohjanmaa',\n",
              "  'Keski-Pohjanmaa',\n",
              "  'Peruskoulut ja lukiot',\n",
              "  'Pohjanmaa',\n",
              "  'Ressun lukio',\n",
              "  'Seinäjoki',\n",
              "  'Vaasa',\n",
              "  'abiturientit',\n",
              "  'immunisaatio',\n",
              "  'injektiot',\n",
              "  'karanteeni',\n",
              "  'koulut',\n",
              "  'lukio',\n",
              "  'rehtorit',\n",
              "  'rokotteet',\n",
              "  'tartuntataudit',\n",
              "  'ylioppilaskirjoitukset'],\n",
              " 'text': 'Maanantaina 13.9. alkavat ylioppilaskirjoitukset ovat jo neljännet ”koronakirjoitukset”. Vaikka yhteiskunta alkaa vähitellen vapauttaa rajoituksia, ylioppilastutkintolautakunnan kanta on edelleen tiukka. \\n – Jos on koronavirukseen liittyviä oireita, ei saa osallistua ylioppilaskirjoituksiin, sanoo ylioppilastutkintolautakunnan pääsihteeri  Taina Tähkä . Myöskään valvojat eivät voi osallistua kokeisiin sairaana tai edes vähäoireisina. \\n Kahdesta koronarokotteesta on kuitenkin hyötyä, jos abiturientti on altistunut koronavirukselle muualla kuin omassa perheessään, ja toisesta rokoteannoksesta on kulunut vähintään viikko. \\n – Jos kokelaalla on kaksi koronarokotusta, häntä ei määrätä altistumiskaranteeniin, vaan hän voi osallistua kirjoituksiin, kertovat Seinäjoen lukion rehtori  Teijo Päkkilä  ja Vaasan lyseon rehtori  Jaakko Perttu . \\n Kaksi koronarokotusta ei vapauta kasvomaskien käytöstä. Helsingin Ressun lukion rehtori  Ari Huovinen  kertoo, että kokelas saa ottaa kokeeseen vaihtomaskeja, käsidesiä sekä läpinäkyvät pussit puhtaille ja likaisille kasvomaskeille. Ressun lukio suosittelee, että eväät tuodaan kotoa ja myös vesipullot täytetään jo kotona. \\n Kokeeseen saapuminen porrastetaan edelleen ruuhkien välttämiseksi. Varsinkin reaaliaineet ja kielet keräävät paljon kirjoittajia myös syksyllä. Esimerkiksi Seinäjoen lukiossa vieraan pitkän kielen ylioppilaskokeeseen osallistuu tänä syksynä kerralla 270 oppilasta. \\n Torppaako jälkioire osallistumisen? \\n Ylen saaman tiedon mukaan esimerkiksi Keski-Pohjanmaalla Kokkolassa osa abiturienteista on aikaistanut toista koronarokotustaan, koska aiemmin saatu rokotusaika olisi ollut koepäivän välittömässä läheisyydessä. Tällä on haluttu varmistaa, etteivät mahdolliset rokotuksen jälkioireet estä kirjoituksiin osallistumista. \\n Jos kokelas saa edellisenä päivänä pistetystä koronarokotteesta oireita, esimerkiksi lämpöä, joka jatkuu vielä koeaamuna, saako hän tulla kirjoituksiin? \\n Helsingin Ressun lukion rehtorin Ari Huovisen ja Vaasan lyseon lukion rehtorin Jaakko Pertun näkemys on yhtä tiukka: Jos on vähänkin oireita, kirjoituksiin ei voi osallistua. \\n – Ei kannata ottaa koronapiikkiä edellisenä päivänä, se on huono ajoitus, Perttu toteaa. \\n Seinäjoella tilannetta arvioidaan joustavammin. \\n – Ne [jälkioireet] alkavat nopeasti rokotuksen jälkeen, ja jos oireet ovat selkästi yhdistettävissä rokotukseen, niin mielestäni se ei estä kirjoituksiin osallistumista, rehtori Teijo Päkkilä miettii. \\n Ylioppilastutkintolautakunta heittää pallon tällaisissa tapauksissa, ja kaikissa koronaoireisiin liittyvissä kysymyksissä paikallisille terveysviranomaisille . \\n – Helsingistä on mahdotonta sanoa, mistä kenenkin lämmönnousu johtuu. Sekä kokelas että lukio voivat kääntyä oman terveydenhuollon puoleen, pääsihteeri Taina Tähkä toteaa. \\n Vaasassa tarjolla myös karanteenisali \\n Osa lukioista on tilojensa puolesta pystynyt järjestämään karanteenissa oleville mahdollisuuden osallistua ylioppilaskirjoituksiin. Yksi tällaisista kouluista on Vaasan lyseon lukio. Rehtori Jaakko Perttu kertoo, että kevään 2021 terveysviranomaisten kanssa luotu käytäntö vaatii järjestelyjä. \\n Karanteeniin määrätyn oppilaan on käytävä koronatestissä edellisenä päivänä. Kokeeseen tullessaan hän ei voi käyttää julksita liikennettä, vaan paikalle on tultava erillisellä kuljetuksella. Karanteenisaliin on oma sisäänkäyntinsä. \\n – Se johtaa eräänlaiseen välitilaan, jossa terveydenhoitoviranomainen tarkistaa edellisen päivän testituloksen, mittaa kokelaan kuumeen ja tarkistaa voinnin, Perttu kertoo. \\n Jos oireita ei ole ja testitulos on negatiivinen, kokelas pääsee tekemään kokeensa karanteenisalissa. \\n Kelpuuttaako Vaasan yhteiskoulun lukio kaupasta tai apteekista ostettavat koronapikatestit testin tekemiseksi? \\n – Vanhemmat ovat tätä kyselleet, mutta vastaukseni on, että koulu ei kelpuuta testejä, sen tekevät tervesviranomaiset. Sairaanhoitopiiri hyväksyy ja tekee testit, kertoo Perttu.',\n",
              " 'timestamp': datetime.datetime(2021, 9, 13, 5, 20),\n",
              " 'title': 'Tuplasti rokotettu tai ei – rajoitukset pysyvät edelleen tiukkoina syksyn ylioppilaskirjoituksissa',\n",
              " 'url': 'https://yle.fi/uutiset/3-12095716'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What NLP tasks could the dataset be used for?"
      ],
      "metadata": {
        "id": "jfaYw-ntmQse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset could be used to train an classifier to classify news articles to different classes, dataset could be used to generate summaries of blocks of texts"
      ],
      "metadata": {
        "id": "Z8HUhFB1rgjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What features does the dataset have?"
      ],
      "metadata": {
        "id": "Qb8ZNg3ImSB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G8_RCZLq-2I",
        "outputId": "3f95d371-4e5b-4d8a-c550-a83366aa3540"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['summary', 'tags', 'text', 'timestamp', 'title', 'url'],\n",
              "        num_rows: 5985\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['summary', 'tags', 'text', 'timestamp', 'title', 'url'],\n",
              "        num_rows: 1497\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset has following features:\n",
        "\n",
        "* summary: Summary of the news article\n",
        "* tags: classes that the news article has been assigned to\n",
        "* text: actual news article\n",
        "* timestamp: time of the news article\n",
        "* title: the title of the news article\n",
        "* url: the location of the news article in the internet"
      ],
      "metadata": {
        "id": "4ZZxYfLxrDtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How many space-separated words do the texts of the dataset contain in total?"
      ],
      "metadata": {
        "id": "4ULcgrwmmt80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splits = ['train', 'test']\n",
        "words = 0\n",
        "\n",
        "for s in splits:\n",
        "  for ex in ds[s]:\n",
        "    ex_l_words = len(ex['text'].split(' '))\n",
        "    words += ex_l_words\n",
        "print(f'Count of space-separated words in the _text_ feature of the dataset entries: {words}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXNM7UqRmu_h",
        "outputId": "048ced41-de16-4b9b-be12-4a1e1cd1ad6b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of space-separated words in the _text_ feature of the dataset entries: 3670842\n"
          ]
        }
      ]
    }
  ]
}